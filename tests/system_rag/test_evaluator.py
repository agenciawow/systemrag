#!/usr/bin/env python3
"""
Testes automatizados para o sistema de avaliaÃ§Ã£o do Sistema RAG Multimodal

Testa o avaliador automÃ¡tico:
- RAGEvaluator
- MÃ©tricas de avaliaÃ§Ã£o
- ConfiguraÃ§Ã£o via ambiente
- GeraÃ§Ã£o de relatÃ³rios
"""

import pytest
import os
import json
import tempfile
import shutil
from typing import Dict, List
from dotenv import load_dotenv

# Carregar variÃ¡veis de ambiente
load_dotenv()

# Imports do sistema
try:
    from rag_evaluator import RAGEvaluator
    from system_rag.search.conversational_rag import ModularConversationalRAG
except ImportError as e:
    pytest.skip(f"MÃ³dulos do avaliador nÃ£o disponÃ­veis: {e}", allow_module_level=True)

class TestEvaluatorConfig:
    """ConfiguraÃ§Ã£o para testes do avaliador"""
    
    # ConfiguraÃ§Ã£o mÃ­nima para testes
    TEST_EVAL_QUESTIONS = "Quais produtos estÃ£o disponÃ­veis?|Qual Ã© o preÃ§o do hambÃºrguer?|VocÃªs tÃªm opÃ§Ãµes vegetarianas?"
    TEST_EVAL_KEYWORDS = "produtos,disponÃ­vel,cardÃ¡pio|preÃ§o,valor,hambÃºrguer|vegetariano,vegano,opÃ§Ã£o"
    TEST_EVAL_CATEGORIES = "catalog|pricing|dietary"
    
    # APIs necessÃ¡rias
    OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')
    VOYAGE_API_KEY = os.getenv('VOYAGE_API_KEY')
    ASTRA_DB_APPLICATION_TOKEN = os.getenv('ASTRA_DB_APPLICATION_TOKEN')
    ASTRA_DB_API_ENDPOINT = os.getenv('ASTRA_DB_API_ENDPOINT')
    
    @property
    def has_full_config(self) -> bool:
        return all([
            self.OPENAI_API_KEY,
            self.VOYAGE_API_KEY,
            self.ASTRA_DB_APPLICATION_TOKEN,
            self.ASTRA_DB_API_ENDPOINT
        ])

@pytest.fixture
def evaluator_config():
    """Fixture com configuraÃ§Ã£o do avaliador"""
    return TestEvaluatorConfig()

@pytest.fixture
def temp_env_vars():
    """Fixture que configura variÃ¡veis de ambiente temporÃ¡rias"""
    config = TestEvaluatorConfig()
    
    # Salvar valores originais
    original_questions = os.environ.get('EVAL_QUESTIONS')
    original_keywords = os.environ.get('EVAL_KEYWORDS')
    original_categories = os.environ.get('EVAL_CATEGORIES')
    
    # Configurar valores de teste
    os.environ['EVAL_QUESTIONS'] = config.TEST_EVAL_QUESTIONS
    os.environ['EVAL_KEYWORDS'] = config.TEST_EVAL_KEYWORDS
    os.environ['EVAL_CATEGORIES'] = config.TEST_EVAL_CATEGORIES
    
    yield
    
    # Restaurar valores originais
    if original_questions:
        os.environ['EVAL_QUESTIONS'] = original_questions
    elif 'EVAL_QUESTIONS' in os.environ:
        del os.environ['EVAL_QUESTIONS']
        
    if original_keywords:
        os.environ['EVAL_KEYWORDS'] = original_keywords
    elif 'EVAL_KEYWORDS' in os.environ:
        del os.environ['EVAL_KEYWORDS']
        
    if original_categories:
        os.environ['EVAL_CATEGORIES'] = original_categories
    elif 'EVAL_CATEGORIES' in os.environ:
        del os.environ['EVAL_CATEGORIES']

@pytest.fixture
def temp_output_dir():
    """Fixture que cria diretÃ³rio temporÃ¡rio para arquivos de saÃ­da"""
    temp_dir = tempfile.mkdtemp()
    original_cwd = os.getcwd()
    os.chdir(temp_dir)
    
    yield temp_dir
    
    os.chdir(original_cwd)
    shutil.rmtree(temp_dir)

class TestRAGEvaluatorInitialization:
    """Testes de inicializaÃ§Ã£o do avaliador"""
    
    @pytest.mark.skipif(not TestEvaluatorConfig().has_full_config, reason="ConfiguraÃ§Ã£o completa nÃ£o disponÃ­vel")
    def test_evaluator_with_rag_instance(self, evaluator_config):
        """Testa inicializaÃ§Ã£o com instÃ¢ncia RAG"""
        try:
            rag_instance = ModularConversationalRAG()
            evaluator = RAGEvaluator(rag_instance)
            assert evaluator.rag_system is not None
            assert evaluator.rag_system == rag_instance
        except Exception as e:
            pytest.skip(f"InicializaÃ§Ã£o com RAG falhou: {e}")
    
    def test_evaluator_without_rag_instance(self, evaluator_config):
        """Testa inicializaÃ§Ã£o sem instÃ¢ncia RAG"""
        evaluator = RAGEvaluator(None)
        assert evaluator.rag_system is None

class TestTestDatasetCreation:
    """Testes de criaÃ§Ã£o do dataset de teste"""
    
    def test_create_test_dataset_from_env(self, evaluator_config, temp_env_vars):
        """Testa criaÃ§Ã£o de dataset a partir de variÃ¡veis de ambiente"""
        evaluator = RAGEvaluator(None)
        dataset = evaluator.create_test_dataset()
        
        assert len(dataset) == 3  # 3 perguntas de teste
        
        # Verificar estrutura do primeiro item
        first_item = dataset[0]
        assert hasattr(first_item, 'question')
        assert hasattr(first_item, 'expected_keywords')
        assert hasattr(first_item, 'category')
        assert hasattr(first_item, 'question_id')
        
        # Verificar conteÃºdo
        assert first_item.question == "Quais produtos estÃ£o disponÃ­veis?"
        assert "produtos" in first_item.expected_keywords
        assert first_item.category == "catalog"
    
    def test_create_test_dataset_empty_env(self, evaluator_config):
        """Testa criaÃ§Ã£o de dataset com variÃ¡veis vazias"""
        # Temporariamente limpar variÃ¡veis
        original_questions = os.environ.get('EVAL_QUESTIONS', '')
        os.environ['EVAL_QUESTIONS'] = ''
        
        try:
            evaluator = RAGEvaluator(None)
            dataset = evaluator.create_test_dataset()
            assert len(dataset) == 0
        finally:
            # Restaurar
            if original_questions:
                os.environ['EVAL_QUESTIONS'] = original_questions
    
    def test_parse_keywords(self, evaluator_config):
        """Testa parsing de palavras-chave"""
        evaluator = RAGEvaluator(None)
        
        # Teste com keywords vÃ¡lidas
        keywords_str = "palavra1,palavra2,palavra3"
        parsed = evaluator._parse_keywords(keywords_str)
        assert parsed == ["palavra1", "palavra2", "palavra3"]
        
        # Teste com string vazia
        parsed_empty = evaluator._parse_keywords("")
        assert parsed_empty == []
        
        # Teste com espaÃ§os
        keywords_with_spaces = "palavra1, palavra2 , palavra3"
        parsed_spaces = evaluator._parse_keywords(keywords_with_spaces)
        assert parsed_spaces == ["palavra1", "palavra2", "palavra3"]

class TestEvaluationMetrics:
    """Testes das mÃ©tricas de avaliaÃ§Ã£o"""
    
    def test_keyword_coverage_calculation(self, evaluator_config):
        """Testa cÃ¡lculo de cobertura de palavras-chave"""
        evaluator = RAGEvaluator(None)
        
        # Teste com cobertura completa
        response = "Temos produtos disponÃ­veis no cardÃ¡pio"
        keywords = ["produtos", "disponÃ­vel", "cardÃ¡pio"]
        coverage = evaluator._calculate_keyword_coverage(response, keywords)
        assert coverage == 1.0  # 100% de cobertura
        
        # Teste com cobertura parcial
        response = "Temos produtos no menu"
        keywords = ["produtos", "disponÃ­vel", "cardÃ¡pio"]
        coverage = evaluator._calculate_keyword_coverage(response, keywords)
        assert coverage == 1/3  # 33% de cobertura (apenas "produtos")
        
        # Teste com nenhuma cobertura
        response = "Texto sem palavras relevantes"
        keywords = ["produtos", "disponÃ­vel", "cardÃ¡pio"]
        coverage = evaluator._calculate_keyword_coverage(response, keywords)
        assert coverage == 0.0  # 0% de cobertura
    
    def test_response_analysis(self, evaluator_config):
        """Testa anÃ¡lise de resposta"""
        evaluator = RAGEvaluator(None)
        
        # Resposta vÃ¡lida
        valid_response = "Temos hambÃºrgueres, batatas fritas e refrigerantes disponÃ­veis"
        is_valid, reason = evaluator._analyze_response(valid_response)
        assert is_valid is True
        assert reason == "Response is valid"
        
        # Resposta indicando nÃ£o encontrado
        not_found_response = "NÃ£o consegui encontrar informaÃ§Ãµes sobre isso"
        is_valid, reason = evaluator._analyze_response(not_found_response)
        assert is_valid is False
        assert "not found" in reason.lower()
        
        # Resposta vazia
        empty_response = ""
        is_valid, reason = evaluator._analyze_response(empty_response)
        assert is_valid is False
        assert "empty" in reason.lower()

class TestEvaluationExecution:
    """Testes de execuÃ§Ã£o da avaliaÃ§Ã£o"""
    
    @pytest.mark.skipif(not TestEvaluatorConfig().has_full_config, reason="ConfiguraÃ§Ã£o completa nÃ£o disponÃ­vel")
    def test_evaluate_single_question(self, evaluator_config, temp_env_vars):
        """Testa avaliaÃ§Ã£o de uma Ãºnica pergunta"""
        try:
            rag_instance = ModularConversationalRAG()
            evaluator = RAGEvaluator(rag_instance)
            
            # Criar pergunta de teste
            test_question = type('TestQuestion', (), {
                'question': 'Quais produtos estÃ£o disponÃ­veis?',
                'expected_keywords': ['produtos', 'disponÃ­vel'],
                'category': 'catalog',
                'question_id': 'test_1'
            })()
            
            result = evaluator._evaluate_single_question(test_question)
            
            # Verificar estrutura do resultado
            assert 'question_id' in result
            assert 'question' in result
            assert 'response' in result
            assert 'response_time' in result
            assert 'keyword_coverage' in result
            assert 'is_valid_response' in result
            assert 'analysis_reason' in result
            
            # Verificar valores
            assert result['question_id'] == 'test_1'
            assert result['response_time'] > 0
            assert 0 <= result['keyword_coverage'] <= 1
            
        except Exception as e:
            pytest.skip(f"AvaliaÃ§Ã£o de pergunta Ãºnica falhou: {e}")
    
    @pytest.mark.skipif(not TestEvaluatorConfig().has_full_config, reason="ConfiguraÃ§Ã£o completa nÃ£o disponÃ­vel")
    def test_run_evaluation(self, evaluator_config, temp_env_vars, temp_output_dir):
        """Testa execuÃ§Ã£o completa da avaliaÃ§Ã£o"""
        try:
            rag_instance = ModularConversationalRAG()
            evaluator = RAGEvaluator(rag_instance)
            
            # Executar avaliaÃ§Ã£o
            report = evaluator.run_evaluation()
            
            # Verificar estrutura do relatÃ³rio
            assert 'evaluation_summary' in report
            assert 'overall_metrics' in report
            assert 'category_analysis' in report
            assert 'detailed_results' in report
            
            # Verificar summary
            summary = report['evaluation_summary']
            assert 'total_questions' in summary
            assert 'successful_evaluations' in summary
            assert 'failed_evaluations' in summary
            assert 'success_rate' in summary
            
            # Verificar mÃ©tricas
            metrics = report['overall_metrics']
            assert 'average_response_time' in metrics
            assert 'average_keyword_coverage' in metrics
            assert 'average_precision' in metrics
            assert 'average_recall' in metrics
            assert 'average_f1_score' in metrics
            
            # Verificar se arquivos foram criados
            assert os.path.exists('rag_evaluation_report.json')
            assert os.path.exists('rag_evaluation_detailed.txt')
            
        except Exception as e:
            pytest.skip(f"AvaliaÃ§Ã£o completa falhou: {e}")

class TestReportGeneration:
    """Testes de geraÃ§Ã£o de relatÃ³rios"""
    
    def test_save_json_report(self, evaluator_config, temp_output_dir):
        """Testa salvamento de relatÃ³rio JSON"""
        evaluator = RAGEvaluator(None)
        
        # Criar dados de teste
        test_report = {
            'evaluation_summary': {
                'total_questions': 3,
                'successful_evaluations': 2,
                'failed_evaluations': 1,
                'success_rate': 0.667
            },
            'overall_metrics': {
                'average_response_time': 5.5,
                'average_keyword_coverage': 0.75
            }
        }
        
        # Salvar relatÃ³rio
        evaluator._save_json_report(test_report)
        
        # Verificar se arquivo foi criado
        assert os.path.exists('rag_evaluation_report.json')
        
        # Verificar conteÃºdo
        with open('rag_evaluation_report.json', 'r', encoding='utf-8') as f:
            loaded_report = json.load(f)
        
        assert loaded_report['evaluation_summary']['total_questions'] == 3
        assert loaded_report['overall_metrics']['average_response_time'] == 5.5
    
    def test_save_detailed_report(self, evaluator_config, temp_output_dir):
        """Testa salvamento de relatÃ³rio detalhado"""
        evaluator = RAGEvaluator(None)
        
        # Criar dados de teste
        test_report = {
            'evaluation_summary': {
                'total_questions': 2,
                'successful_evaluations': 2,
                'failed_evaluations': 0,
                'success_rate': 1.0
            },
            'overall_metrics': {
                'average_response_time': 4.2,
                'average_keyword_coverage': 0.85,
                'average_precision': 0.80,
                'average_recall': 0.75,
                'average_f1_score': 0.77
            },
            'category_analysis': {
                'catalog': {
                    'questions_count': 1,
                    'success_rate': 1.0,
                    'avg_response_time': 4.0
                }
            },
            'detailed_results': [
                {
                    'question_id': 'q1',
                    'question': 'Teste?',
                    'response': 'Resposta teste',
                    'keyword_coverage': 0.8,
                    'is_valid_response': True
                }
            ]
        }
        
        # Salvar relatÃ³rio
        evaluator._save_detailed_report(test_report)
        
        # Verificar se arquivo foi criado
        assert os.path.exists('rag_evaluation_detailed.txt')
        
        # Verificar conteÃºdo bÃ¡sico
        with open('rag_evaluation_detailed.txt', 'r', encoding='utf-8') as f:
            content = f.read()
        
        assert 'RELATÃ“RIO DE AVALIAÃ‡ÃƒO' in content
        assert 'Taxa de sucesso: 100.0%' in content
        assert 'Tempo mÃ©dio: 4.2s' in content

class TestErrorHandling:
    """Testes de tratamento de erros"""
    
    def test_evaluation_with_no_rag_system(self, evaluator_config, temp_env_vars):
        """Testa avaliaÃ§Ã£o sem sistema RAG"""
        evaluator = RAGEvaluator(None)
        
        # Deve retornar erro
        report = evaluator.run_evaluation()
        assert 'error' in report
        assert 'RAG system not provided' in report['error']
    
    def test_evaluation_with_no_questions(self, evaluator_config):
        """Testa avaliaÃ§Ã£o sem perguntas configuradas"""
        # Limpar variÃ¡veis de ambiente
        original_questions = os.environ.get('EVAL_QUESTIONS', '')
        os.environ['EVAL_QUESTIONS'] = ''
        
        try:
            evaluator = RAGEvaluator(None)
            dataset = evaluator.create_test_dataset()
            assert len(dataset) == 0
        finally:
            # Restaurar
            if original_questions:
                os.environ['EVAL_QUESTIONS'] = original_questions
    
    def test_malformed_environment_variables(self, evaluator_config):
        """Testa variÃ¡veis de ambiente malformadas"""
        # Configurar variÃ¡veis inconsistentes
        os.environ['EVAL_QUESTIONS'] = 'Pergunta 1|Pergunta 2'  # 2 perguntas
        os.environ['EVAL_KEYWORDS'] = 'palavra1,palavra2'  # 1 grupo de keywords
        os.environ['EVAL_CATEGORIES'] = 'cat1'  # 1 categoria
        
        try:
            evaluator = RAGEvaluator(None)
            dataset = evaluator.create_test_dataset()
            
            # Deve criar dataset mesmo com inconsistÃªncias, usando defaults
            assert len(dataset) == 2  # Baseado no nÃºmero de perguntas
            
        finally:
            # Limpar
            if 'EVAL_QUESTIONS' in os.environ:
                del os.environ['EVAL_QUESTIONS']
            if 'EVAL_KEYWORDS' in os.environ:
                del os.environ['EVAL_KEYWORDS']
            if 'EVAL_CATEGORIES' in os.environ:
                del os.environ['EVAL_CATEGORIES']

if __name__ == "__main__":
    # Executar testes bÃ¡sicos se chamado diretamente
    import sys
    
    print("ğŸ§ª Executando testes bÃ¡sicos do avaliador...")
    
    config = TestEvaluatorConfig()
    
    # Verificar configuraÃ§Ã£o
    if config.has_full_config:
        print("âœ… ConfiguraÃ§Ã£o completa disponÃ­vel - testes completos habilitados")
    else:
        print("âš ï¸  ConfiguraÃ§Ã£o incompleta - alguns testes serÃ£o pulados")
        print("   Configure: OPENAI_API_KEY, VOYAGE_API_KEY, ASTRA_DB_APPLICATION_TOKEN, ASTRA_DB_API_ENDPOINT")
    
    # Verificar se avaliador estÃ¡ disponÃ­vel
    try:
        from rag_evaluator import RAGEvaluator
        print("âœ… MÃ³dulo rag_evaluator disponÃ­vel")
    except ImportError as e:
        print(f"âŒ MÃ³dulo rag_evaluator nÃ£o disponÃ­vel: {e}")
        sys.exit(1)
    
    # Executar com pytest se disponÃ­vel
    try:
        import pytest
        print("\nğŸš€ Executando testes completos com pytest...")
        pytest.main([__file__, "-v", "-x"])
    except ImportError:
        print("âš ï¸  pytest nÃ£o disponÃ­vel. Instale com: pip install pytest")
        print("âœ… Testes bÃ¡sicos de configuraÃ§Ã£o passaram")